{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPqUyjRVYnBC7B2zUQVq5fj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":25,"metadata":{"id":"34uGcq_TS0yq","executionInfo":{"status":"ok","timestamp":1720146905728,"user_tz":300,"elapsed":1824,"user":{"displayName":"omar leonardo pe√±a garcia","userId":"10817833892713693050"}}},"outputs":[],"source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","#cargar el modelo y el Tokenizador en la CPU por defecto\n","model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\") # Fixed typo in class name and model name\n","tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n","\n","def generar_respuesta(imput_text):\n","  imput_ids = tokenizer.encode(imput_text + tokenizer.eos_token, return_tensors=\"pt\")\n","  bot_imput_ids = model.generate(imput_ids, max_length=1000, pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id) # Fixed arguments to model.generate\n","  return tokenizer.decode(bot_imput_ids[0], skip_special_tokens=True)"]}]}